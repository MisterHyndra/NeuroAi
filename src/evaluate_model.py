"""
Script para avalia√ß√£o detalhada dos modelos de diagn√≥stico de c√¢ncer
"""
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_curve, auc,
    precision_recall_curve, average_precision_score
)
import tensorflow as tf
from tensorflow.keras.models import load_model

class CancerModelEvaluator:
    def __init__(self, model_path):
        """
        Inicializa o avaliador de modelos
        
        Args:
            model_path: Caminho para o modelo salvo
        """
        self.model_path = model_path
        self.model = self.load_model()
        
    def load_model(self):
        """Carrega o modelo salvo"""
        try:
            model = load_model(self.model_path)
            print(f"‚úÖ Modelo carregado de: {self.model_path}")
            return model
        except Exception as e:
            print(f"‚ùå Erro ao carregar modelo: {e}")
            return None
    
    def evaluate_comprehensive(self, X_test, y_test, class_names=None):
        """
        Realiza avalia√ß√£o abrangente do modelo
        
        Args:
            X_test: Dados de teste
            y_test: Labels verdadeiros
            class_names: Nomes das classes
        """
        if self.model is None:
            print("‚ùå Modelo n√£o carregado")
            return None
        
        print("üîç Iniciando avalia√ß√£o abrangente...")
        
        # Fazer predi√ß√µes
        y_pred_proba = self.model.predict(X_test)
        
        # Determinar se √© classifica√ß√£o bin√°ria ou multi-classe
        is_binary = len(y_pred_proba.shape) == 1 or y_pred_proba.shape[1] == 1
        
        if is_binary:
            y_pred_classes = (y_pred_proba > 0.5).astype(int).flatten()
            y_test_classes = y_test.astype(int) if len(y_test.shape) == 1 else y_test.flatten()
        else:
            y_pred_classes = np.argmax(y_pred_proba, axis=1)
            y_test_classes = np.argmax(y_test, axis=1) if len(y_test.shape) > 1 else y_test
        
        # M√©tricas b√°sicas
        test_loss, test_accuracy = self.model.evaluate(X_test, y_test, verbose=0)
        
        # Relat√≥rio de classifica√ß√£o
        if class_names is None:
            if is_binary:
                class_names = ['Benigno', 'Maligno']
            else:
                class_names = [f'Classe {i}' for i in range(y_pred_proba.shape[1])]
        
        report = classification_report(
            y_test_classes, y_pred_classes, 
            target_names=class_names, 
            output_dict=True
        )
        
        # Matriz de confus√£o
        cm = confusion_matrix(y_test_classes, y_pred_classes)
        
        # Compilar resultados
        results = {
            'test_loss': test_loss,
            'test_accuracy': test_accuracy,
            'classification_report': report,
            'confusion_matrix': cm,
            'y_true': y_test_classes,
            'y_pred': y_pred_classes,
            'y_pred_proba': y_pred_proba,
            'class_names': class_names,
            'is_binary': is_binary
        }
        
        print(f"üìä Acur√°cia: {test_accuracy:.4f}")
        print(f"üìä Loss: {test_loss:.4f}")
        
        return results
    
    def plot_confusion_matrix(self, results, save_path='results/detailed_confusion_matrix.png'):
        """Plota matriz de confus√£o detalhada"""
        cm = results['confusion_matrix']
        class_names = results['class_names']
        
        plt.figure(figsize=(10, 8))
        
        # Calcular percentuais
        cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
        
        # Criar anota√ß√µes
        annotations = []
        for i in range(cm.shape[0]):
            row = []
            for j in range(cm.shape[1]):
                row.append(f'{cm[i,j]}\n({cm_percent[i,j]:.1f}%)')
            annotations.append(row)
        
        sns.heatmap(cm, annot=annotations, fmt='', cmap='Blues',
                   xticklabels=class_names, yticklabels=class_names)
        
        plt.title('Matriz de Confus√£o Detalhada')
        plt.xlabel('Predi√ß√£o')
        plt.ylabel('Verdadeiro')
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"‚úÖ Matriz de confus√£o salva em: {save_path}")
    
    def plot_roc_curves(self, results, save_path='results/roc_curves.png'):
        """Plota curvas ROC"""
        if not results['is_binary']:
            print("‚ö†Ô∏è Curvas ROC implementadas apenas para classifica√ß√£o bin√°ria")
            return
        
        y_true = results['y_true']
        y_scores = results['y_pred_proba'].flatten()
        
        # Calcular ROC
        fpr, tpr, _ = roc_curve(y_true, y_scores)
        roc_auc = auc(fpr, tpr)
        
        plt.figure(figsize=(10, 8))
        plt.plot(fpr, tpr, color='darkorange', lw=2, 
                label=f'ROC curve (AUC = {roc_auc:.3f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('Taxa de Falsos Positivos')
        plt.ylabel('Taxa de Verdadeiros Positivos')
        plt.title('Curva ROC - Diagn√≥stico de C√¢ncer')
        plt.legend(loc="lower right")
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"‚úÖ Curva ROC salva em: {save_path}")
        print(f"üìä AUC Score: {roc_auc:.3f}")
        
        return roc_auc
    
    def plot_precision_recall_curve(self, results, save_path='results/precision_recall_curve.png'):
        """Plota curva Precision-Recall"""
        if not results['is_binary']:
            print("‚ö†Ô∏è Curva PR implementada apenas para classifica√ß√£o bin√°ria")
            return
        
        y_true = results['y_true']
        y_scores = results['y_pred_proba'].flatten()
        
        # Calcular Precision-Recall
        precision, recall, _ = precision_recall_curve(y_true, y_scores)
        avg_precision = average_precision_score(y_true, y_scores)
        
        plt.figure(figsize=(10, 8))
        plt.plot(recall, precision, color='blue', lw=2,
                label=f'PR curve (AP = {avg_precision:.3f})')
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Curva Precision-Recall - Diagn√≥stico de C√¢ncer')
        plt.legend(loc="lower left")
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"‚úÖ Curva PR salva em: {save_path}")
        print(f"üìä Average Precision: {avg_precision:.3f}")
        
        return avg_precision
    
    def analyze_errors(self, results, X_test, num_examples=8):
        """Analisa erros de classifica√ß√£o"""
        y_true = results['y_true']
        y_pred = results['y_pred']
        y_pred_proba = results['y_pred_proba']
        
        # Encontrar erros
        error_indices = np.where(y_true != y_pred)[0]
        
        if len(error_indices) == 0:
            print("üéâ Nenhum erro encontrado!")
            return
        
        print(f"‚ùå Total de erros: {len(error_indices)} de {len(y_true)} ({len(error_indices)/len(y_true)*100:.1f}%)")
        
        # Selecionar alguns exemplos para visualiza√ß√£o
        num_examples = min(num_examples, len(error_indices))
        selected_errors = np.random.choice(error_indices, num_examples, replace=False)
        
        # Plotar erros
        fig, axes = plt.subplots(2, 4, figsize=(20, 10))
        axes = axes.ravel()
        
        for i, idx in enumerate(selected_errors):
            if i >= len(axes):
                break
                
            axes[i].imshow(X_test[idx])
            
            true_label = results['class_names'][y_true[idx]]
            pred_label = results['class_names'][y_pred[idx]]
            
            if results['is_binary']:
                confidence = y_pred_proba[idx][0] if y_pred_proba[idx][0] > 0.5 else 1 - y_pred_proba[idx][0]
            else:
                confidence = np.max(y_pred_proba[idx])
            
            axes[i].set_title(f'Verdadeiro: {true_label}\nPredito: {pred_label}\nConfian√ßa: {confidence:.2f}')
            axes[i].axis('off')
        
        plt.tight_layout()
        plt.savefig('results/error_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("‚úÖ An√°lise de erros salva em: results/error_analysis.png")
    
    def generate_detailed_report(self, results, save_path='results/detailed_evaluation_report.txt'):
        """Gera relat√≥rio detalhado da avalia√ß√£o"""
        
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write("RELAT√ìRIO DETALHADO DE AVALIA√á√ÉO\n")
            f.write("=" * 50 + "\n\n")
            
            # M√©tricas gerais
            f.write("M√âTRICAS GERAIS\n")
            f.write("-" * 20 + "\n")
            f.write(f"Acur√°cia: {results['test_accuracy']:.4f}\n")
            f.write(f"Loss: {results['test_loss']:.4f}\n\n")
            
            # Relat√≥rio de classifica√ß√£o
            f.write("RELAT√ìRIO DE CLASSIFICA√á√ÉO\n")
            f.write("-" * 30 + "\n")
            
            report = results['classification_report']
            for class_name in results['class_names']:
                if class_name in report:
                    metrics = report[class_name]
                    f.write(f"\n{class_name.upper()}:\n")
                    f.write(f"  Precision: {metrics['precision']:.3f}\n")
                    f.write(f"  Recall: {metrics['recall']:.3f}\n")
                    f.write(f"  F1-Score: {metrics['f1-score']:.3f}\n")
                    f.write(f"  Support: {metrics['support']}\n")
            
            # M√©tricas macro e weighted
            f.write(f"\nM√âTRICAS AGREGADAS:\n")
            if 'macro avg' in report:
                macro = report['macro avg']
                f.write(f"Macro Avg - Precision: {macro['precision']:.3f}, Recall: {macro['recall']:.3f}, F1: {macro['f1-score']:.3f}\n")
            if 'weighted avg' in report:
                weighted = report['weighted avg']
                f.write(f"Weighted Avg - Precision: {weighted['precision']:.3f}, Recall: {weighted['recall']:.3f}, F1: {weighted['f1-score']:.3f}\n")
            
            # Matriz de confus√£o
            f.write(f"\nMATRIZ DE CONFUS√ÉO\n")
            f.write("-" * 20 + "\n")
            cm = results['confusion_matrix']
            f.write(f"Classes: {results['class_names']}\n")
            f.write(f"{cm}\n")
        
        print(f"üìÑ Relat√≥rio detalhado salvo em: {save_path}")

def main():
    """Fun√ß√£o principal para avalia√ß√£o"""
    
    print("üìä Avalia√ß√£o Detalhada do Modelo de Diagn√≥stico de C√¢ncer")
    print("=" * 60)
    
    # Verificar se existem modelos salvos
    model_dir = "models"
    if not os.path.exists(model_dir):
        print("‚ùå Diret√≥rio de modelos n√£o encontrado!")
        print("Execute primeiro o treinamento: python src/train_model.py")
        return
    
    # Listar modelos dispon√≠veis
    model_files = [f for f in os.listdir(model_dir) if f.endswith('.h5')]
    
    if not model_files:
        print("‚ùå Nenhum modelo encontrado!")
        print("Execute primeiro o treinamento: python src/train_model.py")
        return
    
    print("üìÅ Modelos dispon√≠veis:")
    for i, model_file in enumerate(model_files):
        print(f"   {i+1}. {model_file}")
    
    # Selecionar modelo
    try:
        choice = int(input("\nEscolha o modelo para avaliar (n√∫mero): ")) - 1
        if 0 <= choice < len(model_files):
            selected_model = model_files[choice]
        else:
            print("‚ùå Escolha inv√°lida")
            return
    except ValueError:
        print("‚ùå Por favor digite um n√∫mero v√°lido")
        return
    
    model_path = os.path.join(model_dir, selected_model)
    print(f"üîç Avaliando modelo: {selected_model}")
    
    # Carregar dados de teste (voc√™ precisa implementar isso)
    # Por enquanto, vamos usar dados sint√©ticos para demonstra√ß√£o
    print("‚ö†Ô∏è Usando dados sint√©ticos para demonstra√ß√£o")
    print("   Para usar dados reais, carregue seus dados de teste aqui")
    
    # Dados sint√©ticos para demonstra√ß√£o
    X_test = np.random.rand(100, 224, 224, 3)
    y_test = np.random.randint(0, 2, 100)
    
    try:
        # Criar avaliador
        evaluator = CancerModelEvaluator(model_path)
        
        # Realizar avalia√ß√£o abrangente
        results = evaluator.evaluate_comprehensive(X_test, y_test)
        
        if results:
            # Plotar visualiza√ß√µes
            evaluator.plot_confusion_matrix(results)
            
            if results['is_binary']:
                evaluator.plot_roc_curves(results)
                evaluator.plot_precision_recall_curve(results)
            
            # Analisar erros
            evaluator.analyze_errors(results, X_test)
            
            # Gerar relat√≥rio detalhado
            evaluator.generate_detailed_report(results)
            
            print("\nüéâ Avalia√ß√£o conclu√≠da!")
            print(f"üìä Resumo: Acur√°cia = {results['test_accuracy']:.2%}")
    
    except Exception as e:
        print(f"‚ùå Erro durante avalia√ß√£o: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()